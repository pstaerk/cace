#!/usr/bin/env python
# coding: utf-8

import sys
sys.path.append('../cace/')

import numpy as np
import torch
import torch.nn as nn
import logging

import cace
from cace.representations import Cace
from cace.modules import CosineCutoff, MollifierCutoff, PolynomialCutoff
from cace.modules import BesselRBF, GaussianRBF, GaussianRBFCentered

from cace.models.atomistic import NeuralNetworkPotential
from cace.tasks.train import TrainingTask

torch.set_default_dtype(torch.float32)

cace.tools.setup_logger(level='INFO')
cutoff = 5.

logging.info("reading data")
# collection = cace.tasks.get_dataset_from_xyz(train_path='../../training_set/H2O_BEC.xyz',
collection = cace.tasks.get_dataset_from_xyz(train_path='/work/pstaerk/datasets_ml/water_clusters/training_data.xyz',
                                 valid_fraction=0.1,
                                 seed=1,
                                 cutoff=cutoff,
                                 data_key={'energy': 'energy', 'forces':'forces'}, 
                                 atomic_energies={1: -5.868579157459375, 8: -2.9342895787296874}
                                 )
batch_size = 2

train_loader = cace.tasks.load_data_loader(collection=collection,
                              data_type='train',
                              batch_size=batch_size,
                              )

valid_loader = cace.tasks.load_data_loader(collection=collection,
                              data_type='valid',
                              batch_size=4,
                              )

use_device = 'cuda'
device = cace.tools.init_device(use_device)
logging.info(f"device: {use_device}")


logging.info("building CACE representation")
radial_basis = BesselRBF(cutoff=cutoff, n_rbf=6, trainable=True)
#cutoff_fn = CosineCutoff(cutoff=cutoff)
cutoff_fn = PolynomialCutoff(cutoff=cutoff)

cace_representation = Cace(
    zs=[1,8],
    n_atom_basis=2,
    embed_receiver_nodes=True,
    cutoff=cutoff,
    cutoff_fn=cutoff_fn,
    radial_basis=radial_basis,
    n_radial_basis=12,
    max_l=3,
    max_nu=3,
    num_message_passing=0,
    type_message_passing=['Bchi'],
    args_message_passing={'Bchi': {'shared_channels': False, 'shared_l': False}},
    #avg_num_neighbors=1,
    device=device,
    timeit=False
           )

cace_representation.to(device)
logging.info(f"Representation: {cace_representation}")

sr_energy = cace.modules.atomwise.Atomwise(n_layers=3,
                                         output_key='SR_energy',
                                         n_hidden=[32,16],
                                         use_batchnorm=False,
                                         add_linear_nn=True)



q = cace.modules.Atomwise(
    n_layers=3,
    n_hidden=[24,12],
    n_out=1,
    per_atom_output_key='q',
    output_key = 'tot_q',
    residual=False,
    add_linear_nn=True,
    bias=False)

# ep = cace.modules.EwaldPotential(dl=2,
#                     sigma=1.0,
#                     feature_key='q',
#                     output_key='ewald_potential',
#                     remove_self_interaction=False,
#                    aggregation_mode='sum')
ep = cace.modules.CoulombPotential(feature_key='q',
                                   output_key='coulomb_potential',
                                   aggregation_mode='sum')

e_add = cace.modules.FeatureAdd(feature_keys=['SR_energy', 'coulomb_potential'],
                                output_key='CACE_energy')

forces = cace.modules.Forces(energy_key='CACE_energy',
                             forces_key='CACE_forces')

# polarization = cace.modules.Polarization(pbc=True, phase_key='phase') #, output_index=2)
# grad = cace.modules.Grad(
#     y_key = 'polarization',
#     x_key = 'positions',
#     output_key = 'bec_complex',
#     #output_key = 'bec'
# )
# dephase = cace.modules.Dephase(
#     input_key = 'bec_complex',
#     phase_key = 'phase',
#     output_key = 'CACE_bec'
# )

cace_nnp = NeuralNetworkPotential(
    representation=cace_representation,
    output_modules=[sr_energy, q, ep, e_add, forces], #, polarization, grad, dephase],
    keep_graph=True
)

cace_nnp.to(device)


logging.info(f"First train loop:")
energy_loss = cace.tasks.GetLoss(
    target_name='energy',
    predict_name='CACE_energy',
    loss_fn=torch.nn.MSELoss(),
    loss_weight=0.1
)

force_loss = cace.tasks.GetLoss(
    target_name='forces',
    predict_name='CACE_forces',
    loss_fn=torch.nn.MSELoss(),
    loss_weight=1000
)

# bec_loss = cace.tasks.GetLoss(
#     target_name='bec',
#     predict_name='CACE_bec',
#     loss_fn=torch.nn.MSELoss(),
#     loss_weight=10
# )

from cace.tools import Metrics

e_metric = Metrics(
    target_name='energy',
    predict_name='CACE_energy',
    name='e/atom',
    per_atom=True
)

f_metric = Metrics(
    target_name='forces',
    predict_name='CACE_forces',
    name='f'
)

# bec_metric = Metrics(
#     target_name='bec',
#     predict_name='CACE_bec',
#     name='bec'
# )

# Example usage
logging.info("creating training task")

optimizer_args = {'lr': 1e-2, 'betas': (0.99, 0.999)}  
scheduler_args = {'step_size': 20, 'gamma': 0.5}

# for i in range(5):
task = TrainingTask(
    model=cace_nnp,
    losses=[energy_loss, force_loss],
    metrics=[e_metric, f_metric],#, bec_metric],
    device=device,
    optimizer_args=optimizer_args,
    scheduler_cls=torch.optim.lr_scheduler.StepLR,
    scheduler_args=scheduler_args,
    max_grad_norm=10,
    ema=False, #True,
    ema_start=10,
    warmup_steps=5,
)

logging.info("training")
print(f"Current VRAM allocated: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB")
print(f"Max VRAM allocated:     {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB")
print(f"Current VRAM reserved:  {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB")
print(f"Max VRAM reserved:      {torch.cuda.max_memory_reserved(device) / 1024**2:.2f} MB")

# Add charge monitoring during training
def monitor_charges(model, loader, device, epoch):
    """Monitor charge predictions during validation"""
    # Save current training state
    was_training = model.training
    model.eval()
    
    all_charges = []
    all_atomic_numbers = []
    total_charge_squared = []
    
    with torch.no_grad():
        for batch in loader:
            # Convert batch to dictionary if needed
            if hasattr(batch, 'to_dict'):
                batch_dict = batch.to_dict()
            elif hasattr(batch, '__dict__'):
                batch_dict = {k: v for k, v in batch.__dict__.items() if not k.startswith('_')}
            else:
                batch_dict = batch
            
            # Move tensors to device
            batch_dict = {key: val.to(device) if isinstance(val, torch.Tensor) else val 
                         for key, val in batch_dict.items()}
            
            # Call model without training flag to avoid gradient computation
            outputs = model(batch_dict, training=False)
            
            charges = outputs['q']  # per-atom charges
            atomic_numbers = batch_dict['atomic_numbers']
            batch_indices = batch_dict.get('batch', None)
            
            all_charges.append(charges.cpu())
            all_atomic_numbers.append(atomic_numbers.cpu())
            
            # Calculate sum of charges squared per structure
            if batch_indices is not None:
                for i in torch.unique(batch_indices):
                    mask = batch_indices == i
                    q_sum_sq = (charges[mask].sum())**2
                    total_charge_squared.append(q_sum_sq.item())
            else:
                # Single structure case
                q_sum_sq = charges.sum()**2
                total_charge_squared.append(q_sum_sq.item())
    
    all_charges = torch.cat(all_charges, dim=0)
    all_atomic_numbers = torch.cat(all_atomic_numbers, dim=0)
    
    # Statistics by element
    logging.info(f"\n=== Charge Statistics (Epoch {epoch}) ===")
    for z in torch.unique(all_atomic_numbers):
        mask = all_atomic_numbers == z
        z_charges = all_charges[mask]
        logging.info(f"Element Z={z.item()}: mean={z_charges.mean():.4f}, "
                    f"std={z_charges.std():.4f}, "
                    f"min={z_charges.min():.4f}, max={z_charges.max():.4f}")
    
    # Overall statistics
    logging.info(f"All atoms: mean={all_charges.mean():.4f}, "
                f"std={all_charges.std():.4f}")
    logging.info(f"Total charge per structure: mean(Q²)={np.mean(total_charge_squared):.6f}, "
                f"std(Q²)={np.std(total_charge_squared):.6f}")
    logging.info("=" * 50 + "\n")
    
    # Restore training state
    if was_training:
        model.train()

# Training loop with periodic charge monitoring
epochs = 40
check_every = 10  # Check charges every 10 epochs

for epoch in range(epochs):
    # Train for one epoch
    task.fit(train_loader, valid_loader, epochs=1, screen_nan=False, print_stride=0)
    
    # Monitor charges periodically
    if (epoch + 1) % check_every == 0 or epoch == 0:
        monitor_charges(cace_nnp, valid_loader, device, epoch + 1)

task.save_model('water-model.pth')
cace_nnp.to(device)

# logging.info(f"Second train loop:")
# energy_loss = cace.tasks.GetLoss(
#     target_name='energy',
#     predict_name='CACE_energy',
#     loss_fn=torch.nn.MSELoss(),
#     loss_weight=1
# )
# 
# # bec_loss = cace.tasks.GetLoss(
# #     target_name='bec',
# #     predict_name='CACE_bec',
# #     loss_fn=torch.nn.MSELoss(),
# #     loss_weight=100
# # )
# 
# task.update_loss([energy_loss, force_loss])
# logging.info("training")
# task.fit(train_loader, valid_loader, epochs=100, screen_nan=False, print_stride=0)
# 
# 
# task.save_model('water-model-2.pth')
# cace_nnp.to(device)
# 
# logging.info(f"Third train loop:")
# energy_loss = cace.tasks.GetLoss(
#     target_name='energy',
#     predict_name='CACE_energy',
#     loss_fn=torch.nn.MSELoss(),
#     loss_weight=10 
# )
# 
# # bec_loss = cace.tasks.GetLoss(
# #     target_name='bec',
# #     predict_name='CACE_bec',
# #     loss_fn=torch.nn.MSELoss(),
# #     loss_weight=1000
# # )
# 
# task.update_loss([energy_loss, force_loss])
# task.fit(train_loader, valid_loader, epochs=100, screen_nan=False, print_stride=0)
# 
# task.save_model('water-model-3.pth')
# 
# logging.info(f"Fourth train loop:")
# energy_loss = cace.tasks.GetLoss(
#     target_name='energy',
#     predict_name='CACE_energy',
#     loss_fn=torch.nn.MSELoss(),
#     loss_weight=1000
# )
# 
# task.update_loss([energy_loss, force_loss])
# task.fit(train_loader, valid_loader, epochs=100, screen_nan=False, print_stride=0)
# 
# task.save_model('water-model-4.pth')
# 
# logging.info(f"Finished")


trainable_params = sum(p.numel() for p in cace_nnp.parameters() if p.requires_grad)
logging.info(f"Number of trainable parameters: {trainable_params}")



